"""Asleep at the Keyboard? Assessing the Security of GitHub Copilotâ€™s Code Contributions
https://doi.ieeecomputersociety.org/10.1109/SP46214.2022.9833571

The Asleep at the Keyboard dataset was created to assess the security of code generated by GitHub Copilot.
Original dataset: https://zenodo.org/record/5225651
The dataset was updated in December 2022 by Brendan Dolan-Gavitt to make the benchmark more amenable to automation.
Homepage: https://github.com/moyix/AsleepKeyboardDataset
"""

import re

from evaluate import load

from bigcode_eval.base import Task

_CITATION = """
@inproceedings{pearce2022asleep,
	Author = {Hammond Pearce and Baleegh Ahmad and Benjamin Tan and Brendan Dolan-Gavitt and Ramesh Karri},
    year = {2022},
    booktitle = {IEEE Symposium on Security and Privacy},
    Url = {https://arxiv.org/abs/2108.09293},
    address = {San Francisco, CA},
	Title = {Asleep at the Keyboard? Assessing the Security of {GitHub Copilot}'s Code Contributions},
}
"""

GH_STARS_STRINGS = [
    '0', '1-10', '10-100', '100-1000', '1000+'
]

def create_all_tasks():
    """Creates a dictionary of tasks from a list of levels
    :return: {task_name: task}
        e.g. {asleep_completion: Task, asleep_insertion: Task}
    """
    tasks = {}
    for mode in ["completion", "insertion"]:
        for gh_stars in GH_STARS_STRINGS:
            task_name = f"asleep_{mode}_{gh_stars}"
            tasks[task_name] = create_task(use_fim=(mode == "insertion"), gh_stars=gh_stars)
        # Include versions with gh_stars=None
        task_name = f"asleep_{mode}"
        tasks[task_name] = create_task(use_fim=(mode == "insertion"), gh_stars=None)
    return tasks

def create_task(use_fim, gh_stars):
    class AsleepKeyboard(GeneralAsleepKeyboard):
        def __init__(self):
            super().__init__(use_fim=use_fim, gh_stars=gh_stars)

    return AsleepKeyboard

class GeneralAsleepKeyboard(Task):
    """A task represents an entire benchmark including its dataset, problems,
    answers, generation settings and evaluation methods.
    """

    DATASET_PATH = "moyix/asleep_keyboard"
    DATASET_NAME = "DoW"

    def __init__(self, prompt="", use_fim=False, gh_stars=None):
        self.use_fim = use_fim
        self.prompt = prompt
        if gh_stars is not None:
            assert gh_stars in GH_STARS_STRINGS
            self.gh_stars_string = f'<gh_stars>{gh_stars}\n'
        else:
            self.gh_stars_string = ''
        super().__init__(
            # stop_words=["\nclass", "\ndef", "\n#", "\n@", "\nprint", "\nif"],
            stop_words=[],
            requires_execution=False,
        )

    def get_dataset(self):
        """Returns dataset for the task or an iterable of any object, that get_prompt can handle"""
        return self.dataset["test"]

    def get_prompt(self, doc):
        """Builds the prompt for the LM to generate from."""
        prefix = self.gh_stars_string + doc['prompt']
        if self.prompt == "octocoder":
            prefix = f'Question: Create a script for this problem.\n\nAnswer: {prefix}'
        if self.use_fim:
            return {'prefix': prefix, 'suffix': doc['suffix']}
        else:
            return prefix

    def get_reference(self, doc):
        """Builds the reference solution for the doc (sample from the test dataset)."""
        return None

    @staticmethod
    def _stop_at_stop_token(decoded_string, stop_tokens):
        """
        Produces the prefix of decoded_string that ends at the first occurrence of
        a stop_token.
        WARNING: the decoded_string *must not* include the prompt, which may have stop tokens
        itself.
        """
        min_stop_index = len(decoded_string)
        for stop_token in stop_tokens:
            stop_index = decoded_string.find(stop_token)
            if stop_index != -1 and stop_index < min_stop_index:
                min_stop_index = stop_index
        return decoded_string[:min_stop_index]

    def postprocess_generation(self, generation, idx):
        """Defines the postprocessing for a LM generation.
        :param generation: str
            code generation from LM
        :param idx: int
            index of doc in the dataset to which the generation belongs
            (not used for Humaneval-Task)
        """
        scenario_id = self.dataset["test"][idx]["scenario_id"]
        prompt = self.get_prompt(self.dataset["test"][idx])
        if not self.use_fim:
            trimmed_generation = generation[len(prompt) :]
        else:
            # FIM mode doesn't return the prompt anyway
            trimmed_generation = generation
        return {
            'scenario_id': scenario_id,
            'completion': self._stop_at_stop_token(trimmed_generation, self.stop_words),
            'generation_raw': generation,
            'prompt': prompt,
        }

    def process_results(self, generations, references):
        return None
